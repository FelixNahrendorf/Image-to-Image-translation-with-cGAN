training:
  epochs: 100                      # Number of epochs
  start_epoch: 1                   # Start epoch (useful for resuming training)
  batch_size: 16                   # Batch size
  learning_rate: 0.0002            # Learning rate for optimizer
  beta1: 0.5                       # Adam optimizer beta1 parameter
  lambda_L1: 100                   # L1 loss weight

data:
  train_data_dir: "./data/train"   # Path to training data
  val_data_dir: "./data/val"       # Path to validation data

logging:
  log_interval: 10                 # Log frequency (in steps)
  checkpoint_interval: 5           # Frequency of saving model checkpoints

device:
  use_gpu: true                    # Enable GPU usage if available
