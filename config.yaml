training:
  epochs: 100                      # Number of epochs
  start_epoch: 1                   # Start epoch (useful for resuming training)
  batch_size: 16                   # Batch size
  learning_rate: 0.0002            # Learning rate for optimizer
  beta1: 0.5                       # Adam optimizer beta1 parameter
  lambda_L1: 100                   # L1 loss weight

data:
  train_images_dir: "data/processed/train/images"
  train_labels_dir: "data/processed/train/labels"
  val_images_dir: "data/processed/val/images"
  val_labels_dir: "data/processed/val/labels"

logging:
  log_interval: 10                 # Log frequency (in steps)
  checkpoint_interval: 5           # Frequency of saving model checkpoints

device:
  use_gpu: true                    # Enable GPU usage if available
